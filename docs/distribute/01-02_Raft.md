#### **一、分布式系统的挑战**

时序性 Timing

并发性 Concurrency

健壮性 Robustness

一致性 Consistency

#### **二、分布式共识 Consensus **

分布式系统中多个节点之间，彼此对某个状态达成一致结果的过程

#### **三、分布式共识的应用 Application of consensus**

<1> 逻辑时间的共识，决定事件发生的顺序

<2> 互斥性的共识，决定谁拥有访问的资源

<3> 协调者的共识，谁是 leader

**数据一致性模型**

- 强一致性
- 弱一致性
  - 不承诺可以立即读到最新写入的值，也不会承诺具体多久之后可以读到；-> 不一致窗口
- 最终一致性

  - 弱一致性的特例
  - 不一致窗口：主要受通信延迟，系统负载和副本个数的影响
    **写一致性级别**

- Any： 所有节点都没写成功，如果请求成功保存到了**失败重传**的缓存队列中，也算成功。any 是最弱的写一致性级别。
- One：必须写成功一个节点；任何一个节点写入成功后，立即返回成功给客户端
- Quorum：多数节点写入成功后
- All：所有节点都写入成功后，返回成功
  **Quorum NWR 算法**

三要素：

- N 副本数
- W 写一致性级别 ： 完成 W 个副本更新，才完成写操作
- R 读一致性级别 ： 读取 R 个副本，选最新的日志作为最新数据

一致性效果：

- W+R>N，强一致性
- W+R<=N，最终一致性
  实际场景选择：

- N 表示副本的冗余备份能力
- W=N 时，读性能比较好（读一个就成功了）
- R=N 时，写性能比较好（写一个就成功了）
- W=R=(N+1)/2 时，容错能力比较好，能容忍（N-1）/2 的故障

#### **cap 理论**

一致性，可用性，分区容错 -- 三角不可能

分布式环境，必定存在网络分区，所以分区容错性必须满足，只能选择两种模型：

- cp
- ap

#### **base 理论**

cap 理论的一种妥协，降低了发生分区容错时，对可用性和一致性的要求

- 基本可用：允许可用性降低
- 软状态：允许系统中数据存在中间状态
- 最终一致性：允许数据同步存在时延

实现基本可用的方法：

- 流量削峰、延迟响应、体验降级、过载保护

- 故障隔离

- 弹性扩容

**paxos 共识算法**

- 分布式系统如何就某个值达成一致
- 一种理论思想
  **basic paxos**

- 提案 ： [n, v]，n 为提案编号，v 为提议值
- prepare 准备请求
- accept 接受请求

三种角色

- proposer (提议者)：提议一个值，用于投票表决
- acceptor（接受者）：对每个提议的值进行投票，并存储接受的值
- learner（记录员）：被告知投票的结果，接受达成共识的值，存储保存，不参与投票的过程。

两个阶段：

- 准备阶段（prepare）

  (a) Proposer 选择一个**提案编号 N**，然后向**半数以上**的 Acceptor 发送编号为 N 的**Prepare 请求**。

  (b) Acceptor 收到一个编号为 N 的 Prepare 请求，且 N**大于**该 Acceptor 已经**响应过的**所有**Prepare 请求**的编号

  那么它就会将它已经**接受过的编号最大的提案（如果有的话）**作为响应反馈给 Proposer，同时该 Acceptor 承诺**不再接受**任何**编号小于 N 的提案**。

- 接受阶段（accept)

  (a) 如果 Proposer 收到**半数以上**Acceptor 对其发出的编号为 N 的 Prepare 请求的**响应**，那么它就会发送一个针对**[N,V]提案**的**Accept 请求**给**半数以上**的 Acceptor。注意：V 就是收到的**响应**中**编号最大的提案的 value**，如果响应中**不包含任何提案**，那么 V 就由 Proposer**自己决定**。

  (b) 如果 Acceptor 收到一个针对编号为 N 的提案的 Accept 请求，只要该 Acceptor**没有**对编号**大于 N**的**Prepare 请求**做出过**响应**，它就**接受该提案**。

总结：

- 二阶段提交
- 容错能力：超过半数通过
- 提案编号的大小代表着优先级，通过的最大编号的提案信息

**数据一致性与 paxos 算法**

- 保持数据的一致性的原则：在一个分布式数据库系统中，如果各节点的初始状态一致，每个节点都执行**相同的操作序列**，那么他们最后能得到一个**一致的状态**。

- Paxos 算法**顺序一致性**保证：

  master 维护一个全局写队列，所有写操作都必须放入这个队列**编号**，就能保证顺序一致性

- master 挂了怎么办？

  1. 同一时刻，只有一个写操作被批准，同时并发的写操作要去争取选票
  2. 只有获得过半数选票的写操作才会被批准（所以永远只会有一个写操作得到批准），其他的写操作竞争失败只好再发起一轮投票
  3. 投票中，所有写操作都被严格编号排序，编号严格递增。
  4. 当一个节点接受了一个编号为 100 的写操作，之后又接受到编号为 99 的写操作（因为网络延迟等很多不可预见原因），它马上能意识到自己 数据不一致了，自动停止对外服务并重启同步过程。
  5. 任何一个节点挂掉都不会影响整个集群的数据一致性（总`2n+1`台，除非挂掉大于 n 台）。

**Multi-Paxos**

- 是一种思想，不是算法
- 选举领导者
- 优化机制：当领导者处于稳定状态时，省掉准备阶段，直接进入接受阶段，优化了`basic paxos`
- 不推荐设计和实现新的 Multi-Paxos 算法，而是建议优先考虑 **Raft 算法**

#### **Raft**

可视化 raft 模型

http://thesecretlivesofdata.com/raft/

**新特性 novel features**

① Strong leader 强领导者模型

增强了领导者的作用，比如日志只能从 leader rc 到其他的 server

② Leader election 领导者选举

随机计时器，在任何一致性算法都需要的 heartbeats 上增加了少量机制，同时能快速地解决冲突，比如选票瓜分

③ Membership changes 成员变更

**复制状态机 Replicated state machines**

![1752522-b2e0dc4401aefd3d](https://muyids.oss-cn-beijing.aliyuncs.com/1752522-b2e0dc4401aefd3d.png)

#### 3 subproblems

- Leader election: a new leader must be chosen when an existing leader fails

- Log replication: the leader must accept log entries from clients and replicate them across the cluster,

forcing the other logs to agree with its own

- members change :

#### 算法设计

一、状态 State

<1> 所有服务器上的持久化状态

currentTerm 当前任期

votedFor 投票给的 candidatedId

Log[] 状态机指令，当 log entry 被 leader 确认后还包含 Term

<2> 所有服务器上的不稳定状态

commitIndex : 被提交的最高 log

lastApplied: 应用到状态机的最高 log entry

<3>leader 上的不稳定状态

(竞选成功后 重新初始化)

nextIndex[] ：servers 向服务器发送的下一条 log entry 数据

matchIndex[] ：每一台服务器复制了 leader 的日志记录列表

二、RequestVote RPC

候选人调用来收集选票

请求

term ： 候选人任期

candidateId ： 候选人 id

lastLogIndex : 候选人最新的

lastLogTerm : 候选人最新的 log entry 的 term

三、AppendEntries RPC

Leader 调用去复制 log entry; 也被用来作为心跳

**领导者选举**

两个超时机制

<1>竞选超时 election timeout

<2>心跳超时

关于超时时间

1. 集群刚启动的时候，心跳会超时，这时候 follower 是直接成为 candidator 么？还是还要在心跳超时的基础上等待一个竞选超时才变成 candidator？

   心跳超时后，Follower **等待自身竞选超时**后成为 Candidate 身份并发起选举；

   一般是 150~300ms，使每个 server 的超时时间不一样，这样就避免了多个 Candidate 同时发起选举的问题；

2. 成为 candidator 并且发送投票请求给 follower，这个 candidator 这时候本身等待投票是有个超时的，这个超时也是竞选超时么？

   不是，选举也有时限，规定时间内没有获取到足够多的票数，则当前 Leader 选举竞选失败；但这个时限不是 election timeout ，election timeout 是指心跳超时后发起竞选的时限。

3. 心跳超时和竞选超时哪个大？心跳超时一般设置为多少？

   心跳超时可以自己配置，看你自己的网络规模和拓扑；竞选超时是随机的，大概在几十到几百毫秒；通常情况下，心跳超时比竞选超时长得多，心跳是秒级，竞选是毫秒级。

**日志复制 Log Replication**

服务器节点 log entry 可能出现的情况

![1752522-fc1352afc54b5ce7](https://muyids.oss-cn-beijing.aliyuncs.com/1752522-fc1352afc54b5ce7.png)

How to replication?

##### 网络分区 network partition

---

##### **In Search of an Understandable Consensus Algorithm**

翻译版本

https://www.cnblogs.com/linbingdong/p/6442673.html

**最小选举超时**。在分布式系统中，有时候需要对集群中的成员数量进行更新的操作。对于被删除的服务器而言，如果它们没有及时关闭，那么它们将不会接收到心跳信息和日志信息，从而不断发生超时，最后导致任期不断增加（高于集群中所有成员的任期），然后不断向集群中发送请求投票消息。集群中的 Leader 将变为 Follower，集群中将不断开始新的选举，从而扰乱集群的正常运行。

解决方案：Raft 引入了一个最小选举超时时间，意思是如果集群中存在 Leader 时，并且接收到心跳信息之后在最小选举超时时间内接受到请求投票消息，那么将会忽略掉该投票消息。

---

https://raft.github.io/

---

Raft 协议的博士论文 CONSENSUS: BRIDGING THEORY AND PRACTICE;

用 raft 协议构建一个复制状态机
